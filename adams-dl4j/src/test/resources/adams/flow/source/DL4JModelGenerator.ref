--> dumpfile.txt
---
backprop: true
backpropType: "Standard"
confs:
- iterationCount: 0
  l1ByParam: {}
  l2ByParam: {}
  layer: !<dense>
    activationFn: !<TanH> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.1
    dist: null
    dropOut: 0.0
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    l1: 0.0
    l1Bias: 0.0
    l2: 1.0E-4
    l2Bias: 0.0
    layerName: "layer0"
    learningRate: 0.1
    learningRateSchedule: null
    momentum: NaN
    momentumSchedule: null
    nin: 1
    nout: 3
    rho: NaN
    rmsDecay: NaN
    updater: "SGD"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam: {}
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 6
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables: []
- iterationCount: 0
  l1ByParam: {}
  l2ByParam: {}
  layer: !<dense>
    activationFn: !<TanH> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.1
    dist: null
    dropOut: 0.0
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    l1: 0.0
    l1Bias: 0.0
    l2: 1.0E-4
    l2Bias: 0.0
    layerName: "layer1"
    learningRate: 0.1
    learningRateSchedule: null
    momentum: NaN
    momentumSchedule: null
    nin: 3
    nout: 3
    rho: NaN
    rmsDecay: NaN
    updater: "SGD"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam: {}
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 6
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables: []
- iterationCount: 0
  l1ByParam: {}
  l2ByParam: {}
  layer: !<output>
    activationFn: !<Softmax> {}
    adamMeanDecay: NaN
    adamVarDecay: NaN
    biasInit: 0.0
    biasLearningRate: 0.1
    dist: null
    dropOut: 0.0
    epsilon: NaN
    gradientNormalization: "None"
    gradientNormalizationThreshold: 1.0
    l1: 0.0
    l1Bias: 0.0
    l2: 1.0E-4
    l2Bias: 0.0
    layerName: "layer2"
    learningRate: 0.1
    learningRateSchedule: null
    lossFn: !<NegativeLogLikelihood> {}
    lossFunction: "NEGATIVELOGLIKELIHOOD"
    momentum: NaN
    momentumSchedule: null
    nin: 3
    nout: 1
    rho: NaN
    rmsDecay: NaN
    updater: "SGD"
    weightInit: "XAVIER"
  leakyreluAlpha: 0.0
  learningRateByParam: {}
  learningRatePolicy: "None"
  lrPolicyDecayRate: NaN
  lrPolicyPower: NaN
  lrPolicySteps: NaN
  maxNumLineSearchIterations: 5
  miniBatch: true
  minimize: true
  numIterations: 1
  optimizationAlgo: "STOCHASTIC_GRADIENT_DESCENT"
  pretrain: false
  seed: 6
  stepFunction: null
  useDropConnect: false
  useRegularization: true
  variables: []
inputPreProcessors: {}
iterationCount: 0
pretrain: false
tbpttBackLength: 20
tbpttFwdLength: 20


